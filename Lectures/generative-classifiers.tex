\section{Generative Classifiers and Naive Bayes}

\subsection{Bayes Decision Rule (0/1 Loss)}
Assume the data-generating distribution $P(X,Y)$ is known. To minimize $0/1$ loss, predict
\[
h(x)=\arg\max_{y\in\mathcal{Y}} P(y\mid x).
\]
Using Bayes' rule,
\[
P(y\mid x)=\frac{P(x\mid y)P(y)}{P(x)}
\quad\Longrightarrow\quad
h(x)=\arg\max_{y} P(x\mid y)P(y).
\]

\subsection{Generative Modeling vs.\ Discriminative Modeling}
\begin{itemize}
    \item \textbf{Generative:} model $P(x, y)$ (joint distribution), then use Bayes rule to get $P(y \mid x)$.
    \item \textbf{Conditional / discriminative:} model $P(y\mid x)$ directly (e.g.\ logistic regression).
    \item \textbf{ERM:} pick $h$ to minimize empirical loss on training data.
\end{itemize}

\subsection{Multivariate (Bernoulli) Naive Bayes}
Assume $x\in\{0,1\}^d$ and conditional independence of features given the class:
\[
P(x\mid y)=\prod_{j=1}^d P(x_j\mid y).
\]
The classifier is
\[
h(x)=\arg\max_{y\in\{+1,-1\}} P(y)\prod_{j=1}^d P(x_j\mid y).
\]
In log-space:
\[
h(x)=\arg\max_{y}\left[\log P(y) + \sum_{j=1}^d \log P(x_j\mid y)\right].
\]

\paragraph{Estimators and Smoothing}
\begin{itemize}
    \item \textbf{Estimating $P(Y)$:} Use the fraction of positive/negative examples in the training data:
    \[
    \hat{P}(Y = y) = \frac{m_y}{m}
    \]
    where $m_y$ is the number of training examples in class $y$ and $m$ is the total number of examples.
    \item \textbf{Estimating $P(X \mid Y)$:} 
    \begin{itemize}
        \item \textbf{Maximum Likelihood Estimate:}
        \[
        \hat{P}(X_i = x \mid Y = y) = \frac{\#(X_i = x, y)}{m_y}
        \]
        where $\#(X_i = x, y)$ is the number of training examples in class $y$ where feature $X_i$ takes value $x$.
        \item \textbf{Laplace Smoothing:}
        \[
        \hat{P}(X_i = x \mid Y = y) = \frac{\#(X_i = x, y) + 1}{m_y + |X_i|}
        \]
        where $|X_i|$ is the number of different values that feature $X_i$ can take.
    \end{itemize}
\end{itemize}

\paragraph{Connection to Linear Classification}

\[
h_{\text{naive}}(\vec{x}) = \mathrm{sign} \left[ \ln \frac{P(Y=+1)}{P(Y=-1)} + \sum_{i=1}^N \ln \frac{P(X_i = x_i \mid Y=+1)}{P(X_i = x_i \mid Y=-1)} \right]
\]
This shows that the Naive Bayes classifier is a linear classifier in the feature space, i.e.,
\[
h(\vec{x}) = \mathrm{sign}(\vec{w} \cdot \vec{x} + b)
\]
where
\[
b = \ln \frac{P(Y=+1)}{P(Y=-1)}, \quad
\vec{w}_i = \ln \frac{P(X_i = x_i \mid Y=+1)}{P(X_i = x_i \mid Y=-1)}
\]

\subsection{Multinomial Naive Bayes (Text)}
For documents represented as a bag of words, assume each word is drawn i.i.d.\ from a class-specific distribution over the vocabulary.
If a document has word counts $c(w)$, then
\[
P(x\mid y)\propto \prod_{w\in\mathcal{V}} P(w\mid y)^{c(w)},
\]
and classification again uses $\arg\max_y P(y)P(x\mid y)$.

\subsection{Modeling Continuous Variables: Linear Discriminant Analysis (LDA)}

Assume the feature vectors of each class come from a spherical Gaussian distribution:
\[
P(X = \vec{x} \mid Y = y) = \mathcal{N}(\vec{\mu}_y, \Sigma_y)
\]
where
\[
P(X = \vec{x} \mid Y = y) = \frac{1}{(2\pi)^{N/2} |\Sigma_y|^{1/2}} \exp\left(-\frac{1}{2} (\vec{x} - \vec{\mu}_y)^T \Sigma_y^{-1} (\vec{x} - \vec{\mu}_y)\right)
\]
For LDA, we assume $\Sigma_+ = \Sigma_- = \Sigma = \mathrm{diag}(1)$ (features are normalized to variance 1).

\paragraph{Classification Rule}
\[
h_{\text{LDA}}(\vec{x}) = \arg\max_{y \in \mathcal{Y}} \left[ P(X = \vec{x} \mid Y = y) P(Y = y) \right]
\]
This simplifies to:
\[
h_{\text{LDA}}(\vec{x}) = \arg\max_{y \in \mathcal{Y}} \left[ \ln P(Y = y) - \frac{1}{2} \|\vec{x} - \vec{\mu}_y\|^2 \right]
\]
which is a linear classifier:
\[
h(\vec{x}) = \mathrm{sign}(\vec{w} \cdot \vec{x} + b)
\]
where
\[
\vec{w} = \vec{\mu}_+ - \vec{\mu}_-, \quad
b = \ln \frac{P(Y=+1)}{P(Y=-1)} - \frac{1}{2} (\vec{\mu}_+ + \vec{\mu}_-) \cdot (\vec{\mu}_+ - \vec{\mu}_-)
\]

\paragraph{Parameter Estimation}
\begin{itemize}
    \item \textbf{Class priors:}
    \[
    \hat{P}(Y = +1) = \frac{m_+}{m}, \quad \hat{P}(Y = -1) = \frac{m_-}{m}
    \]
    \item \textbf{Class means:}
    \[
    \vec{\mu}_+ = \frac{1}{m_+} \sum_{i: y_i = +1} \vec{x}_i, \quad
    \vec{\mu}_- = \frac{1}{m_-} \sum_{i: y_i = -1} \vec{x}_i
    \]
\end{itemize}