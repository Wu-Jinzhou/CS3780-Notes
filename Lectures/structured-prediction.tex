\section{Structured Prediction}

\subsection{Predicting Structured Outputs}
In structured prediction, the output $y$ is a complex object rather than a single label/number, e.g.
\begin{itemize}
    \item Multi-label classification (predict a set of labels)
    \item Speech transcription (predict a sentence)
    \item Semantic segmentation (predict a label for each pixel)
    \item Part-of-speech (POS) tagging (predict a tag sequence)
\end{itemize}

\subsection{Supervised Structured Prediction}
We are given data $S=\{(x_i,y_i)\}_{i=1}^m$ where $y_i\in\mathcal{Y}$ is structured and a loss $\Delta(y,\hat{y})$.
The Bayes decision rule for $0/1$ loss is
\[
h(x)=\arg\max_{y\in\mathcal{Y}} P(x,y) = \arg\max_{y\in\mathcal{Y}} P(y\mid x).
\]

\subsection{Hidden Markov Models (HMMs) for Sequence Labeling}
For a sequence $x=(x_1,\dots,x_T)$ and tags $y=(y_1,\dots,y_T)$:
\begin{itemize}
    \item \textbf{Start probabilities:} $P(y_1)$
    \item \textbf{Transitions:} $P(y_t\mid y_{t-1})$ (Markov assumption)
    \item \textbf{Emissions:} $P(x_t\mid y_t)$
\end{itemize}
The joint distribution factorizes as
\[
P(x,y)=P(y_1)P(x_1\mid y_1)\prod_{t=2}^T P(y_t\mid y_{t-1})P(x_t\mid y_t).
\]

\paragraph{Estimating HMM Parameters}
Using maximum likelihood from labeled sequences:
\begin{itemize}
    \item $P(y_1=s)=\dfrac{\#\text{ sequences starting in }s}{\#\text{ sequences}}$
    \item $P(y_t=s\mid y_{t-1}=s')=\dfrac{\#(s'\to s)}{\#(s'\text{ occurs})}$
    \item $P(x_t=o\mid y_t=s)=\dfrac{\#(o \text{ emitted in } s)}{\#(s\text{ occurs})}$
\end{itemize}
Smoothing (as in Naive Bayes) is often needed.

\paragraph{HMM Assumptions}
\begin{itemize}
    \item \textbf{Markov assumption:} The next state depends only on the previous state.
    \begin{itemize}
        \item Example (POS Tagging): The next tag depends only on the previous tag, not on earlier/later tags or words.
    \end{itemize}
    \item \textbf{Emission assumption:} The current emission depends only on the current state.
    \begin{itemize}
        \item Example (POS Tagging): The current word depends only on the current tag, not on earlier/later tags or words.
    \end{itemize}
\end{itemize}

\subsection{Inference: The Viterbi Algorithm}
To efficiently compute the most likely sequence of tags $y = (y_1, \ldots, y_l)$ for a given observation sequence $x = (x_1, \ldots, x_l)$, we want:
\[
\hat{y} = \arg\max_{y \in \{y_1, \ldots, y_l\}} \left\{ P(y_1)P(x_1 \mid y_1) \prod_{i=2}^l P(x_i \mid y_i)P(y_i \mid y_{i-1}) \right\}
\]
Directly maximizing over all possible sequences is computationally infeasible for large $l$, since the number of possible sequences grows exponentially.

\textbf{Viterbi Algorithm:}  
The Viterbi algorithm is a dynamic programming method to efficiently find the most likely sequence. It works by recursively computing the probability of the most likely path ending in each state at each position.

Define the Viterbi score $\delta_y(i)$ as the probability of the most likely sequence of states ending in state $y$ at position $i$:
\[
\delta_y(1) = P(Y_1 = y) P(X_1 = x_1 \mid Y_1 = y)
\]
\[
\delta_y(i+1) = \max_{v \in \{s_1, \ldots, s_k\}} \delta_v(i) P(Y_{i+1} = y \mid Y_i = v) P(X_{i+1} = x_{i+1} \mid Y_{i+1} = y)
\]
At each step, we keep track of the maximizing previous state (the "backpointer") for each state and position.

\textbf{Backtracking:}  
After filling in the dynamic programming table, we recover the most likely sequence by tracing back the maximizing states from the final position.

\textbf{Complexity:}  
The Viterbi algorithm runs in $O(l |S|^2)$ time, where $l$ is the sequence length and $|S|$ is the number of possible states/tags.


\subsection{Beyond Sequences}
More general structured prediction includes graph labeling problems (e.g.\ Markov random fields), where inference can be more complex than the chain-structured Viterbi case.
