\section{Neural Networks}

\subsection{Multi-Layer Neural Networks}

\begin{center}
\includegraphics[width=0.6\textwidth]{Images/nn.png}
\end{center}

A multi-layer neural network (also called a feedforward neural network) consists of an input layer, one or more hidden layers, and an output layer. Each layer is made up of nodes (neurons), and each node in a layer is connected to every node in the next layer.
\begin{itemize}
    \item Let $\vec{v}_i$ be the vector of values in layer $i$.
    \item Let $\vec{w}_{i,j}$ be the vector of weights from layer $i$ to the $j$th node of layer $i+1$.
    \item The value at node $j$ in layer $i+1$ is computed as:
    \[
    v_{i+1, j} = \sigma_{i+1}(\vec{v}_i \cdot \vec{w}_{i,j})
    \]
    where $\sigma_{i+1}$ is the activation function for layer $i+1$.
\end{itemize}

\paragraph{Concise Matrix Formulation}

\begin{itemize}
    \item Let $W_i$ be the weight matrix for layer $i$, where each column $j$ is the weight vector $\vec{w}_{i,j}$.
    \item Layers are fully connected; any missing edge has weight $0$.
    \item The vector of activations for layer $i+1$ is:
    \[
    \vec{v}_{i+1} = \sigma_{i+1}(W_i^\top \vec{v}_i)
    \]
    where $\sigma_{i+1}$ is applied elementwise.
\end{itemize}


The output of a $d$-layer neural network can be written as a nested composition of linear transformations and activation functions:
\[
\sigma_d\left(W_{d-1}^\top \cdots\, \sigma_3\left(W_2^\top\, \sigma_2\left(W_1^\top\, \sigma_1\left(W_0^\top\, \vec{v}_0\right)\right)\right)\cdots\right)
\]
where each $\sigma_i$ is applied elementwise, and $W_i$ is the weight matrix for layer $i$.

\begin{algobox}
\textbf{Forward Propagation Algorithm:}

\textbf{Input:} Neural Network with weight matrices $W_0, W_1, \ldots, W_{d-1}$, activation functions $\sigma_1, \ldots, \sigma_d$ and instance $\vec{x}$

$\vec{v}_0 = \vec{x}$

\textbf{For} $\ell = 1, \ldots, d$
\begin{itemize}
    \item $\vec{s}_\ell = W_{\ell-1}^\top \vec{v}_{\ell-1}$
    \item $\vec{v}_\ell = \sigma_\ell(\vec{s}_\ell)$
\end{itemize}
\textbf{End For}

\textbf{Output} $\vec{v}_d$
\end{algobox}

\subsection{Non-linear Activation Functions}

Activation functions are applied to the nodes of a hidden layer in a neural network. They introduce non-linearity, allowing the network to learn complex patterns.

\begin{center}
\includegraphics[width=0.6\textwidth]{Images/activations.png}
\end{center}

\begin{itemize}
    \item \textbf{Binary step:} Outputs 0 or 1, not differentiable at $x=0$.
    \item \textbf{Sigmoid:} Smooth, outputs between 0 and 1, can cause vanishing gradients.
    \item \textbf{Tanh:} Outputs between -1 and 1, zero-centered.
    \item \textbf{ReLU:} Simple, efficient, helps with vanishing gradient, but can "die" for negative inputs.
\end{itemize}

Deeper neural networks can express more complex functions only if we use non-linear activation.

If all activation functions are linear, then a multi-layer neural network is equivalent to a single-layer linear model. This is because a linear function of linear functions is still linear.

\textbf{Example:} Suppose the activation in the hidden layer is linear, $\sigma_1(x) = x$, and the output activation is non-linear, e.g., $\sigma_2(x) = \mathrm{sign}(x)$. Then, the output can be written as:
\[
v_{\text{out}} = \mathrm{sign}(\bar{w}_1 v_1 + \bar{w}_2 v_2) = \mathrm{sign}\left(\bar{w}_1(\vec{x} \cdot \vec{w} + b) + \bar{w}_2(\vec{x} \cdot \vec{w}' + b')\right)
\]
\[
= \mathrm{sign}(\vec{z} \cdot \vec{x} + \beta)
\]
where $\vec{z} = \bar{w}_1 \vec{w} + \bar{w}_2 \vec{w}'$ and $\beta = \bar{w}_1 b + \bar{w}_2 b'$.



\subsection{Universal Approximators}

A fundamental result in neural networks is the \textbf{Universal Approximation Theorem}. It states that a feedforward neural network with a single hidden layer (i.e., a depth-2 network) and a sufficiently large number of neurons (width) can approximate any continuous function on $\mathbb{R}^n$ to arbitrary accuracy, given appropriate activation functions (such as sigmoid or ReLU).

\textbf{How large does the hidden layer need to be?}
\begin{itemize}
    \item For boolean functions, the required width can be as large as $\exp(n)$, where $n$ is the input dimension.
    \item If we restrict ourselves to networks of polynomial size (i.e., width and number of parameters grow polynomially with $n$), then:
    \begin{itemize}
        \item We cannot approximate all possible functions.
        \item However, this restriction helps reduce the risk of overfitting.
        \item This tradeoff is known as the \textbf{bias-variance tradeoff}: larger networks can fit more complex functions (low bias, high variance), while smaller networks may generalize better (high bias, low variance).
    \end{itemize}
    \item Instead of fully connected layers, we can use structured networks (e.g., convolutional neural networks) to reduce the number of parameters and exploit structure in the data.
\end{itemize}


\subsection{Convolutional Neural Networks}

\begin{center}
\includegraphics[width=0.9\textwidth]{Images/lenet.png}
\end{center}

If we're using neural networks for image data, for a grayscale (1 channel) image of size $1920 \times 1080$, a fully connected first layer, assuming 1000 hidden units, would require $1920 \times 1080 \times 1000 \approx 2$ billion parameters, which is impractical.

\begin{itemize}
    \item CNNs are designed to process data with a grid-like topology, such as images.
    \item Instead of connecting every input pixel to every neuron in the next layer, CNNs use \textbf{local connections} via small filters (kernels) that slide over the input.
    \item Each filter detects specific features (e.g., edges, textures) by performing a \textbf{convolution} operation.
    \item The same filter (set of weights) is applied across the entire input, enabling \textbf{weight sharing} and reducing the number of parameters.
    \item The result of applying a filter is called a \textbf{feature map}.
    \item Multiple filters are used in each layer to detect different features.
    \item CNNs typically alternate between convolutional layers, non-linear activation functions, and pooling (downsampling) layers.
\end{itemize}

\paragraph{Convolutions} Given an $n \times n$ input matrix and an $f \times f$ filter (kernel), the convolution operation produces an $(n-f+1) \times (n-f+1)$ output matrix. Each entry in the output is computed by sliding the filter over the input, multiplying corresponding entries, and summing the results.

\begin{itemize}
    \item The $(i, j)$-th entry of the output is the sum of pairwise multiplications of the filter and the $f \times f$ submatrix of the input whose top-left entry is at $(i, j)$.
    \item The filter is applied across the input with a specified \textbf{stride} (how many positions the filter moves at each step; stride 1 means move by 1 entry).
    \item Convolutions allow for \textbf{local feature extraction} and \textbf{weight sharing} (the same filter is used everywhere).
\end{itemize}

In a convolution layer, filter's weights and biases are parameters that will be learned.

\textbf{Example:}  
If the input is a $5 \times 5$ matrix and the filter is $3 \times 3$, the output will be $3 \times 3$. Each output entry is computed as the sum of elementwise products between the filter and the corresponding $3 \times 3$ patch of the input.

\paragraph{Padding} 
Convolution shrinks the size of the matrix, losing corner information. We can pad 0's around the original matrix.

If we pad on each side by $p = \frac{f-1}{2}$ 0's (filter size is often odd), after convolution, the matrix size is still $n \times n$.

\paragraph{Pooling} 
Pooling layers reduce the spatial dimensions (width and height) of the input, helping to control overfitting and reduce computation.

\begin{itemize}
    \item \textbf{Max Pooling:} For each region (e.g., $2 \times 2$), output the maximum value.
    \item \textbf{Average Pooling:} For each region, output the average value.
    \item Pooling is typically applied with a stride equal to the pooling window size (e.g., non-overlapping $2 \times 2$ regions).
    \item No parameters are learned in pooling layers.
    \item Pooling helps make the representation approximately invariant to small translations in the input.
\end{itemize}

Pooling is similar to convolution in that it operates on local regions, but instead of a learned filter, it uses a fixed function (max or average).
