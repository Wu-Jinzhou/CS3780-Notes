\section{Clustering}

\subsection{Unsupervised Learning}
In unsupervised learning, we are given unlabeled data $\{x_i\}_{i=1}^n$ and aim to discover structure.
\textbf{Clustering} partitions examples into groups such that:
\begin{itemize}
    \item points within a cluster are similar,
    \item points across clusters are dissimilar.
\end{itemize}

\subsection{Similarity Measures}
Common similarity/distance measures include:
\begin{itemize}
    \item \textbf{Euclidean distance:} $\|x-x'\|_2$
    \item \textbf{$\ell_1$ distance:} $\|x-x'\|_1$
    \item \textbf{Cosine similarity:} $\dfrac{x\cdot x'}{\|x\|\,\|x'\|}$
\end{itemize}

\subsection{K-means Clustering}
\paragraph{Objective}
Assign each point to one of $K$ clusters with centroids $\mu_1,\dots,\mu_K$.
Let $r(i)\in\{1,\dots,K\}$ be the cluster assignment of $x_i$.
The $K$-means objective is:
\[
\min_{\{\mu_k\},\,r}\; \sum_{i=1}^n \left\|x_i - \mu_{r(i)}\right\|_2^2.
\]

\paragraph{Alternating Minimization}
\begin{itemize}
    \item \textbf{Assignment step:} with fixed centroids, set $r(i)=\arg\min_k \|x_i-\mu_k\|_2$.
    \item \textbf{Centroid step:} with fixed assignments, set $\mu_k$ to the mean of points assigned to cluster $k$.
\end{itemize}
These steps monotonically decrease the objective but can get stuck in local minima.

\begin{algobox}
\textbf{K-means (Lloyd's Algorithm):}
\begin{enumerate}
    \item Initialize centroids $\mu_1,\dots,\mu_K$ (e.g.\ randomly).
    \item Repeat until convergence:
    \begin{itemize}
        \item Assign each $x_i$ to its nearest centroid.
        \item Update each centroid to the mean of its assigned points.
    \end{itemize}
\end{enumerate}
\end{algobox}

\subsection{Generative Clustering: Gaussian Mixture Models (GMMs)}
\paragraph{Generative Model}
Assume a datapoint $\mathbf{x}$ is generated as follows:
\begin{itemize}
    \item Choose a cluster $z$ from $\{1, \ldots, K\}$ such that $p(z = k) = \pi_k$
    \item Given $z$, sample $\mathbf{x}$ from a Gaussian: $p(\mathbf{x} \mid z = k) = \mathcal{N}(\mathbf{x} \mid \mu_k, \Sigma_k)$
\end{itemize}

This defines a joint distribution:
\[
p(\mathbf{x}, z) = p(z) p(\mathbf{x} \mid z)
\]
with parameters $\pi_k, \mu_k, \Sigma_k$ for $k = 1, \ldots, K$.

\paragraph{Posterior Inference}
Given a data point $\mathbf{x}$, the probability that $z = k$ is:
\[
p(z = k \mid \mathbf{x}) = \frac{p(\mathbf{x} \mid z = k) p(z = k)}{p(\mathbf{x})}
\]
where
\[
p(\mathbf{x}) = \sum_{k=1}^K \pi_k \mathcal{N}(\mathbf{x} \mid \mu_k, \Sigma_k)
\]

\paragraph{Parameter Estimation}
Maximize the likelihood of the data:
\[
\arg\max_{\pi_1, \ldots, \pi_K,\, \mu_1, \ldots, \mu_K,\, \Sigma_1, \ldots, \Sigma_K} \sum_{n=1}^N \log p(\mathbf{x}^{(n)})
\]
where
\[
p(\mathbf{x}^{(n)}) = \sum_{k=1}^K \pi_k \mathcal{N}(\mathbf{x}^{(n)} \mid \mu_k, \Sigma_k)
\]
No closed form solution $\rightarrow$ use Expectation Maximization (EM).

\paragraph{Expectation Maximization (EM) Algorithm}
\begin{itemize}
    \item \textbf{Initialization:} randomly initialize parameters.
    \item \textbf{E-step:} Compute soft assignments (responsibilities):
    \[
    r_k^{(n)} = p(z^{(n)} = k \mid \mathbf{x}^{(n)}) = \frac{\pi_k \mathcal{N}(\mathbf{x}^{(n)} \mid \mu_k, \Sigma_k)}{\sum_{j=1}^K \pi_j \mathcal{N}(\mathbf{x}^{(n)} \mid \mu_j, \Sigma_j)}
    \]
    \item \textbf{M-step:} Update parameters using responsibilities:
    \[
    \mu_k = \frac{\sum_{n=1}^N r_k^{(n)} \mathbf{x}^{(n)}}{\sum_{n=1}^N r_k^{(n)}}, \qquad
    \pi_k = \frac{1}{N} \sum_{n=1}^N r_k^{(n)}, \qquad
    \Sigma_k = \frac{\sum_{n=1}^N r_k^{(n)} (\mathbf{x}^{(n)} - \mu_k)(\mathbf{x}^{(n)} - \mu_k)^\top}{\sum_{n=1}^N r_k^{(n)}}
    \]
    \item Repeat E and M steps until convergence.
\end{itemize}