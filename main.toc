\contentsline {section}{\numberline {1}Supervised Learning and KNN}{2}{section.1}%
\contentsline {subsection}{\numberline {1.1}Supervised Learning}{2}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}K-Nearest Neighbors (KNN)}{2}{subsection.1.2}%
\contentsline {subsection}{\numberline {1.3}Weighted KNN}{3}{subsection.1.3}%
\contentsline {subsection}{\numberline {1.4}Similarity Measures}{3}{subsection.1.4}%
\contentsline {subsection}{\numberline {1.5}Types of Attributes}{4}{subsection.1.5}%
\contentsline {subsection}{\numberline {1.6}Properties of KNN}{4}{subsection.1.6}%
\contentsline {section}{\numberline {2}Inductive Learning and Decision Trees}{5}{section.2}%
\contentsline {subsection}{\numberline {2.1}Inductive Learning}{5}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}Version Space}{5}{subsection.2.2}%
\contentsline {subsection}{\numberline {2.3}List-Then-Eliminate Algorithm}{5}{subsection.2.3}%
\contentsline {subsection}{\numberline {2.4}Decision Trees}{6}{subsection.2.4}%
\contentsline {subsection}{\numberline {2.5}Top-Down Induction of Decision Trees (IDT)}{6}{subsection.2.5}%
\contentsline {subsection}{\numberline {2.6}Choosing the Best Split}{6}{subsection.2.6}%
\contentsline {subsection}{\numberline {2.7}Properties of Decision Trees}{7}{subsection.2.7}%
\contentsline {section}{\numberline {3}Prediction and Overfitting}{8}{section.3}%
\contentsline {section}{\numberline {4}Model Selection and Assessment}{10}{section.4}%
\contentsline {subsection}{\numberline {4.1}Validation Sample}{10}{subsection.4.1}%
\contentsline {paragraph}{Two Nested Learning Algorithms}{10}{section*.20}%
\contentsline {paragraph}{Typical ML Experiment}{10}{section*.21}%
\contentsline {subsection}{\numberline {4.2}Cross-Validation}{11}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}Generalization Error of Hypothesis}{11}{subsection.4.3}%
\contentsline {subsection}{\numberline {4.4}Significance Testing with the Binomial Distribution}{11}{subsection.4.4}%
\contentsline {subsection}{\numberline {4.5}Normal Confidence Intervals}{12}{subsection.4.5}%
\contentsline {subsection}{\numberline {4.6}Hoeffding Bound for Generalization Error}{12}{subsection.4.6}%
\contentsline {subsection}{\numberline {4.7}McNemar's Test}{12}{subsection.4.7}%
\contentsline {section}{\numberline {5}Linear Classifiers}{14}{section.5}%
\contentsline {subsection}{\numberline {5.1}Vectors and Hyperplanes}{14}{subsection.5.1}%
\contentsline {paragraph}{Euclidean Embeddings}{14}{section*.22}%
\contentsline {paragraph}{Euclidean Norm}{14}{section*.23}%
\contentsline {paragraph}{Dot Product}{14}{section*.24}%
\contentsline {paragraph}{Angle \& Projection}{14}{section*.25}%
\contentsline {paragraph}{Hyperplanes}{14}{section*.27}%
\contentsline {subsection}{\numberline {5.2}Linear Classifiers}{14}{subsection.5.2}%
\contentsline {paragraph}{Decision Boundaries in Different Dimensions}{15}{section*.28}%
\contentsline {paragraph}{Homogenous Linear Classifiers}{15}{section*.29}%
\contentsline {paragraph}{Consistent Linear Classifiers}{15}{section*.30}%
\contentsline {paragraph}{Margin}{15}{section*.31}%
\contentsline {subsection}{\numberline {5.3}Perceptron Algorithm}{16}{subsection.5.3}%
\contentsline {section}{\numberline {6}Perceptron}{17}{section.6}%
\contentsline {section}{\numberline {7}Support Vector Machines}{18}{section.7}%
\contentsline {subsection}{\numberline {7.1}Margin of a Linear Classifier}{18}{subsection.7.1}%
\contentsline {subsection}{\numberline {7.2}Computing Optimal Hyperplanes}{18}{subsection.7.2}%
\contentsline {subsection}{\numberline {7.3}Hard Margin SVM}{19}{subsection.7.3}%
\contentsline {subsection}{\numberline {7.4}Soft Margin SVM}{19}{subsection.7.4}%
\contentsline {section}{\numberline {8}Kernels and Duality}{20}{section.8}%
\contentsline {paragraph}{Non-linear Problems}{20}{section*.32}%
\contentsline {paragraph}{SVM with Feature Map}{20}{section*.33}%
\contentsline {subsection}{\numberline {8.1}Dual Perceptron}{20}{subsection.8.1}%
\contentsline {paragraph}{(Batch) Perceptron Algorithm}{20}{section*.34}%
\contentsline {paragraph}{Primal vs. Dual Representation}{21}{section*.35}%
\contentsline {paragraph}{What is Duality good for?}{21}{section*.36}%
\contentsline {subsection}{\numberline {8.2}Dual SVM}{21}{subsection.8.2}%
\contentsline {paragraph}{Leave-One-Out Error and Support Vectors}{22}{section*.37}%
\contentsline {subsection}{\numberline {8.3}Non-Linear Rules through Kernels}{22}{subsection.8.3}%
\contentsline {paragraph}{Kernel Trick}{22}{section*.38}%
\contentsline {paragraph}{Polynomial Kernel Example}{23}{section*.39}%
\contentsline {paragraph}{SVM with Kernel}{23}{section*.40}%
\contentsline {subsection}{\numberline {8.4}Designing Kernels}{23}{subsection.8.4}%
\contentsline {paragraph}{Definition:}{23}{section*.41}%
\contentsline {paragraph}{How to Construct Kernels}{24}{section*.42}%
\contentsline {paragraph}{Theorem:}{24}{section*.43}%
\contentsline {subsection}{\numberline {8.5}Properties of SVMs with Kernels}{24}{subsection.8.5}%
\contentsline {section}{\numberline {9}Regularized Linear Models}{25}{section.9}%
\contentsline {subsection}{\numberline {9.1}ERM Learning}{25}{subsection.9.1}%
\contentsline {subsection}{\numberline {9.2}Bayes Decision Rule (0/1 loss)}{25}{subsection.9.2}%
\contentsline {subsection}{\numberline {9.3}Decision via Bayes Risk}{25}{subsection.9.3}%
\contentsline {subsection}{\numberline {9.4}Learning Conditional Probabilities}{26}{subsection.9.4}%
\contentsline {subsection}{\numberline {9.5}Logistic Regression Model (binary $y\in \{-1,+1\}$)}{26}{subsection.9.5}%
\contentsline {subsection}{\numberline {9.6}Logistic Regression Training (Conditional MLE)}{26}{subsection.9.6}%
\contentsline {subsection}{\numberline {9.7}Regularized Logistic Regression: Probabilistic View}{26}{subsection.9.7}%
\contentsline {subsection}{\numberline {9.8}Regularized Logistic Regression: Summary}{27}{subsection.9.8}%
\contentsline {subsection}{\numberline {9.9}Linear Regression Model}{27}{subsection.9.9}%
\contentsline {subsection}{\numberline {9.10}Linear Regression Training (Conditional MLE)}{27}{subsection.9.10}%
\contentsline {subsection}{\numberline {9.11}Ridge Regression (MAP for a Gaussian prior)}{27}{subsection.9.11}%
\contentsline {subsection}{\numberline {9.12}Discriminative Training: A Unifying View}{28}{subsection.9.12}%
\contentsline {subsection}{\numberline {9.13}``45 ML Algorithms on 1 Slide'': Building Blocks}{28}{subsection.9.13}%
\contentsline {section}{\numberline {10}Optimization with Gradient Descent}{29}{section.10}%
\contentsline {section}{\numberline {11}Neural Networks}{30}{section.11}%
\contentsline {subsection}{\numberline {11.1}Multi-Layer Neural Networks}{30}{subsection.11.1}%
\contentsline {paragraph}{Concise Matrix Formulation}{30}{section*.44}%
\contentsline {subsection}{\numberline {11.2}Non-linear Activation Functions}{31}{subsection.11.2}%
\contentsline {subsection}{\numberline {11.3}Universal Approximators}{32}{subsection.11.3}%
